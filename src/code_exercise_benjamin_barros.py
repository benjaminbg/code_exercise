# -*- coding: utf-8 -*-
"""code_exercise_benjamin_barros.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xFCfbfA4AALZu65oWNMdGQLxAD1NLr7h
"""


import json
import os

from google.colab import drive
from google.colab import auth
drive.mount("/content/drive")
auth.authenticate_user()

import pandas as pd
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.model_selection import RandomizedSearchCV
from sklearn.metrics import accuracy_score, confusion_matrix
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
import pickle
from google.cloud import storage

class DataFormatterTransformer(BaseEstimator, TransformerMixin):
    '''
    Transformer that format the original json into a usable dataframe

    Args:
        (BaseEstimator, TransformerMixin): needed for compatibility 
        with scikit pipelines

    Attributes:
        used_likely_word_list (list): stores most common words for 
        used descriptions
    '''    
    def __init__(self):
        self.used_likely_word_list = ['open box', 'semi nuevo', 'poco uso',
                                      'funciona','perfecto', 'impecable', 
                                      'optimo', 'garantia', 'mes uso',
                                      'meses uso'	,'excelente', 
                                      'leer descripcion']
        pass
    
    '''returns a set containg all unique payment methods for a row'''
    def process_payment_methods(self,series):
        item_list = []
        for items in series:
            item_list.append([value for key, value in items.items() if key == "description"])
        return set([item for sublist in item_list for item in sublist])

    '''Create new columns for the different payment methods'''
    def extract_payment_methods(self,df):
      df_copy = df.copy()
      df_copy["non_mercado_pago_payment_methods_exploded"] = df_copy["non_mercado_pago_payment_methods"].apply(
          lambda x: self.process_payment_methods(x))
      df_copy['pm_efectivo'] = df_copy["non_mercado_pago_payment_methods_exploded"].apply(
          lambda x: 'Efectivo' in x)
      tarjetas_credito = ['Tarjeta de cr√©dito', 'Diners', 'Visa', 'American Express',
                          'Visa Electron', 'Mastercard Maestro','MasterCard']
      df_copy['pm_tarjeta'] = df_copy["non_mercado_pago_payment_methods_exploded"].apply(
          lambda x: any(item in x for item in tarjetas_credito))
      df_copy['pm_transferencia'] = df_copy["non_mercado_pago_payment_methods_exploded"].apply(
          lambda x: 'Transferencia bancaria' in x)
      df_copy['pm_mercadoPago'] = df_copy["non_mercado_pago_payment_methods_exploded"].apply(
          lambda x: 'MercadoPago' in x)
      df_copy['pm_acordar_con_el_comprador'] = df_copy["non_mercado_pago_payment_methods_exploded"].apply(
          lambda x: 'Acordar con el comprador' in x)
      del df_copy["non_mercado_pago_payment_methods_exploded"]
      return df_copy

    def fit(self, X, y=None):
        return self

    def transform(self, X):
        X_copy = X.copy()
        X_copy = self.extract_payment_methods(X_copy)
        X_copy["title_contains_likely_used_word"] = X_copy["title"].apply(
            lambda x: any(word in x.lower() for word in self.used_likely_word_list)
            )

        columns_to_delete = ['date_created','last_updated','original_price',
                             'category_id','international_delivery_mode',
                             'shipping.free_methods','differential_pricing',
                             'official_store_id','deal_ids','permalink','subtitle',
                             'catalog_product_id','video_id','secure_thumbnail',
                             'title','thumbnail','id', 'shipping.methods',
                             'seller_address.city.name','seller_address.state.name',
                             'seller_address.country.name','pictures', 'descriptions',
                             'listing_source', 'coverage_areas','warranty',
                             'sub_status','non_mercado_pago_payment_methods',
                             'seller_id','variations','shipping.dimensions',
                             'attributes','tags', 'parent_item_id','shipping.tags']

        for column in columns_to_delete:
          del X_copy[column]

        return X_copy


class LabelEncoderTransformer(BaseEstimator, TransformerMixin):
    '''
    Transformer that encode categorial variables
    
    Args:
        (BaseEstimator, TransformerMixin): needed for compatibility 
        with scikit pipelines
    
    Attributes:
        categorical_columns (list): columns to transform
    '''    
    def __init__(self):
        self.label_encoders = {}
        self.categorical_columns = ['status','currency_id','seller_address.city.id',
                                    'seller_address.state.id','seller_address.country.id',
                                    'site_id','listing_type_id','shipping.mode', 
                                    'buying_mode', 'shipping.free_shipping']

    def fit(self, X, y=None):
        label_encoded_X = X.copy()
        for column in self.categorical_columns:
            le = LabelEncoder()
            label_encoded_X[column] = le.fit(X[column])
            self.label_encoders[column] = le
        return self

    def transform(self, X, y=None):
        label_encoded_X = X.copy()
        for column in self.categorical_columns:
            le = self.label_encoders[column]
            label_encoded_X[column] = le.transform(X[column])
        return label_encoded_X

preprocessor = ColumnTransformer(
    transformers=[
        ('cat', LabelEncoderTransformer()),  # Label Encoding for categorical columns
    ],
    remainder='passthrough'  # Pass through any remaining columns
)

# You can safely assume that `build_dataset` is correctly implemented
def build_dataset():
    data_path = (
        "/content/drive/My Drive/Colab Notebooks/data/MLA_100k_checked_v3.jsonlines"
    )
    data = [json.loads(x) for x in open(data_path)]
    target = lambda x: x.get("condition")
    N = -10000
    X_train = data[:N]
    X_test = data[N:]
    y_train = [target(x) for x in X_train]
    y_test = [target(x) for x in X_test]
    for x in X_train:
        del x["condition"]
    for x in X_test:
        del x["condition"]
    return X_train, y_train, X_test, y_test


if __name__ == "__main__":
    print("Loading dataset...")
    # Train and test data following sklearn naming conventions
    # X_train (X_test too) is a list of dicts with information about each item.
    # y_train (y_test too) contains the labels to be predicted (new or used).
    # The label of X_train[i] is y_train[i].
    # The label of X_test[i] is y_test[i].
    X_train, y_train, X_test, y_test = build_dataset()


    # Insert your code below this line:
    y_train = [1 if value=='new' else 0 for value in y_train]
    y_test = [1 if value=='new' else 0 for value in y_test]

    X_train = pd.json_normalize(X_train)
    X_test = pd.json_normalize(X_test)

    #Initiate all steps for the pipeline
    formatter = DataFormatterTransformer()
    classifier = RandomForestClassifier()
    preprocessor = ColumnTransformer(
        transformers=[
            ('cat', OneHotEncoder(sparse_output=False, handle_unknown='ignore'), [
                'status','currency_id','seller_address.city.id','seller_address.state.id',
                'seller_address.country.id','site_id','listing_type_id','shipping.mode',
                'buying_mode', 'shipping.free_shipping'])
        ],
        remainder='passthrough'  # Pass through any remaining columns
    )
    
    #Pipeline is created
    pipeline = Pipeline([
        ('formatter',formatter ),
        ('preprocessor', preprocessor),
        ('classifier', classifier)
    ])
    
    #Param grid for parameter optimization
    param_grid = {
        'classifier__n_estimators': [70,80,120,150],
        'classifier__max_depth': [None, 1, 10, 25, 50, 80],
        'classifier__min_samples_split': [2,5,10,40,100],
        'classifier__max_features': ['sqrt','log2',1,5,10,25,50,75],
        'classifier__min_samples_leaf': [1, 3, 9, 15, 30],
        'classifier__ccp_alpha': [0,0.05,0.1,0.2]
    }
    
    #Model is optimized using gridsearch on the full pipeline
    grid_search = RandomizedSearchCV(
        pipeline, param_distributions=param_grid, 
        cv=3, n_jobs=3, verbose=1,n_iter=50)
    
    grid_search.fit(X_train, y_train)
    best_model = grid_search.best_estimator_
    
    y_pred = best_model.predict(X_test)
    # Evaluate the model using accuracy on test
    accuracy = accuracy_score(y_test, y_pred)
    print(f"Accuracy: {accuracy:.2f}")
    
    y_pred_train = best_model.predict(X_train)
    # Evaluate the model using accuracy on train
    accuracy = accuracy_score(y_train, y_pred_train)
    print(f"Accuracy: {accuracy:.2f}")
    
    #TODO: Model is overfitting, try to find a way to regularize
    
    conf_matrix = confusion_matrix(y_test, y_pred)
    conf_matrix_df = pd.DataFrame(
        conf_matrix, index=['Actual 0', 'Actual 1'], 
        columns=['Predicted 0', 'Predicted 1'])
    
    tn, fp, fn, tp = conf_matrix.ravel()
    specificity = tn / (tn + fp)
    print(f"Specificity: {specificity:.2f}")
    
    #Model is serialized
    client = storage.Client()
    bucket_name = 'code_exercise'
    local_file_path = 'code_exercise_meli.pkl'
    cloud_storage_path = 'code_exercise_model.pkl'  
    bucket = client.get_bucket(bucket_name)
    
    # Save model artifact to local filesystem
    artifact_filename = 'code_exercise_model.pkl'
    with open(artifact_filename, 'wb') as model_file:
      pickle.dump(best_model, model_file)
    
    # Upload model artifact to Cloud Storage
    model_directory = 'gs:///code_exercise'
    storage_path = os.path.join(model_directory, artifact_filename)
    
    # Upload the file
    blob = bucket.blob(cloud_storage_path)
    blob.upload_from_filename(local_file_path)